{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "module4-ridge-regression/ridge-regression-assignment",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quinn-dougherty/AB-Demo/blob/master/module4-ridge-regression/ridge_regression_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "YP3OBbg-l0S6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Resources & stretch goals:\n",
        "- https://www.quora.com/What-is-regularization-in-machine-learning\n",
        "- https://blogs.sas.com/content/subconsciousmusings/2017/07/06/how-to-use-regularization-to-prevent-model-overfitting/\n",
        "- https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/\n",
        "- https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n",
        "- https://stats.stackexchange.com/questions/111017/question-about-standardizing-in-ridge-regression#111022\n",
        "\n",
        "Stretch goals:\n",
        "- Revisit past data you've fit OLS models to, and see if there's an `alpha` such that ridge regression results in a model with lower MSE on a train/test split\n",
        "- Yes, Ridge can be applied to classification! Check out [sklearn.linear_model.RidgeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier), and try it on a problem you previous approached with a different classifier (note - scikit LogisticRegression also automatically penalizes based on the $L^2$ norm, so the difference won't be as dramatic)\n",
        "- Implement your own function to calculate the full cost that ridge regression is optimizing (the sum of squared residuals + `alpha` times the sum of squared coefficients) - this alone won't fit a model, but you can use it to verify cost of trained models and that the coefficients from the equivalent OLS (without regularization) may have a higher cost"
      ]
    },
    {
      "metadata": {
        "id": "a3iwP1T0lpIC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.testing import assert_almost_equal\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.preprocessing import StandardScaler, scale\n",
        "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import altair as alt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7AOgpyLQlp0T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment\n",
        "\n",
        "Following is data describing characteristics of blog posts, with a target feature of how many comments will be posted in the following 24 hours.\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/BlogFeedback\n",
        "\n",
        "Investigate - you can try both linear and ridge. You can also sample to smaller data size and see if that makes ridge more important. Don't forget to scale!\n",
        "\n",
        "Focus on the training data, but if you want to load and compare to any of the test data files you can also do that.\n",
        "\n",
        "Note - Ridge may not be that fundamentally superior in this case. That's OK! It's still good to practice both, and see if you can find parameters or sample sizes where ridge does generalize and perform better.\n",
        "\n",
        "When you've fit models to your satisfaction, answer the following question:\n",
        "\n",
        "```\n",
        "Did you find cases where Ridge performed better? If so, describe (alpha parameter, sample size, any other relevant info/processing). If not, what do you think that tells you about the data?\n",
        "```\n",
        "\n",
        "You can create whatever plots, tables, or other results support your argument. In this case, your target audience is a fellow data scientist, *not* a layperson, so feel free to dig in!"
      ]
    },
    {
      "metadata": {
        "id": "ee77qbHzl6mL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.testing import assert_almost_equal\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.preprocessing import StandardScaler, scale\n",
        "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import altair as alt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DiHsqPxhl6ti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1389
        },
        "outputId": "c1847307-6b8e-4e77-c3b8-7a65ab0fa09b"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Data Set Information:\n",
        "\n",
        "This data originates from blog posts. The raw HTML-documents\n",
        "of the blog posts were crawled and processed.\n",
        "The prediction task associated with the data is the prediction\n",
        "of the number of comments in the upcoming 24 hours. In order\n",
        "to simulate this situation, we choose a basetime (in the past)\n",
        "and select the blog posts that were published at most\n",
        "72 hours before the selected base date/time. Then, we calculate\n",
        "all the features of the selected blog posts from the information\n",
        "that was available at the basetime, therefore each instance\n",
        "corresponds to a blog post. The target is the number of\n",
        "comments that the blog post received in the next 24 hours\n",
        "relative to the basetime.\n",
        "\n",
        "In the train data, the basetimes were in the years\n",
        "2010 and 2011. In the test data the basetimes were\n",
        "in February and March 2012. This simulates the real-world\n",
        "situtation in which training data from the past is available\n",
        "to predict events in the future.\n",
        "\n",
        "The train data was generated from different basetimes that may\n",
        "temporally overlap. Therefore, if you simply split the train\n",
        "into disjoint partitions, the underlying time intervals may\n",
        "overlap. Therefore, the you should use the provided, temporally\n",
        "disjoint train and test splits in order to ensure that the\n",
        "evaluation is fair.\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "1...50:\n",
        "Average, standard deviation, min, max and median of the\n",
        "Attributes 51...60 for the source of the current blog post\n",
        "With source we mean the blog on which the post appeared.\n",
        "For example, myblog.blog.org would be the source of\n",
        "the post myblog.blog.org/post_2010_09_10\n",
        "51: Total number of comments before basetime\n",
        "52: Number of comments in the last 24 hours before the\n",
        "basetime\n",
        "53: Let T1 denote the datetime 48 hours before basetime,\n",
        "Let T2 denote the datetime 24 hours before basetime.\n",
        "This attribute is the number of comments in the time period\n",
        "between T1 and T2\n",
        "54: Number of comments in the first 24 hours after the\n",
        "publication of the blog post, but before basetime\n",
        "55: The difference of Attribute 52 and Attribute 53\n",
        "56...60:\n",
        "The same features as the attributes 51...55, but\n",
        "features 56...60 refer to the number of links (trackbacks),\n",
        "while features 51...55 refer to the number of comments.\n",
        "61: The length of time between the publication of the blog post\n",
        "and basetime\n",
        "62: The length of the blog post\n",
        "63...262:\n",
        "The 200 bag of words features for 200 frequent words of the\n",
        "text of the blog post\n",
        "263...269: binary indicator features (0 or 1) for the weekday\n",
        "(Monday...Sunday) of the basetime\n",
        "270...276: binary indicator features (0 or 1) for the weekday\n",
        "(Monday...Sunday) of the date of publication of the blog\n",
        "post\n",
        "277: Number of parent pages: we consider a blog post P as a\n",
        "parent of blog post B, if B is a reply (trackback) to\n",
        "blog post P.\n",
        "278...280:\n",
        "Minimum, maximum, average number of comments that the\n",
        "parents received\n",
        "281: The target: the number of comments in the next 24 hours\n",
        "(relative to basetime)'''\n",
        "\n",
        "zipurl='https://archive.ics.uci.edu/ml/machine-learning-databases/00304/BlogFeedback.zip' \n",
        "\n",
        "!wget unzip https://archive.ics.uci.edu/ml/machine-learning-databases/00304/BlogFeedback.zip\n",
        "  \n",
        "!unzip BlogFeedback\n",
        "#!ls \n",
        "\n",
        "#!cat blogData_test-2012.02.01.00_00.csv"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-25 01:56:02--  http://unzip/\n",
            "Resolving unzip (unzip)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘unzip’\n",
            "--2019-01-25 01:56:02--  https://archive.ics.uci.edu/ml/machine-learning-databases/00304/BlogFeedback.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2583605 (2.5M) [application/zip]\n",
            "Saving to: ‘BlogFeedback.zip.1’\n",
            "\n",
            "BlogFeedback.zip.1  100%[===================>]   2.46M  3.59MB/s    in 0.7s    \n",
            "\n",
            "2019-01-25 01:56:03 (3.59 MB/s) - ‘BlogFeedback.zip.1’ saved [2583605/2583605]\n",
            "\n",
            "FINISHED --2019-01-25 01:56:03--\n",
            "Total wall clock time: 0.8s\n",
            "Downloaded: 1 files, 2.5M in 0.7s (3.59 MB/s)\n",
            "Archive:  BlogFeedback.zip\n",
            "  inflating: blogData_test-2012.02.01.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.02.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.03.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.04.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.05.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.06.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.07.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.08.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.09.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.10.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.11.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.12.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.13.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.14.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.15.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.16.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.17.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.18.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.19.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.20.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.21.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.22.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.23.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.24.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.25.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.26.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.27.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.28.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.29.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.01.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.02.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.03.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.04.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.05.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.06.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.07.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.08.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.09.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.10.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.11.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.12.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.13.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.14.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.15.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.16.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.17.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.18.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.19.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.20.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.21.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.22.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.23.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.24.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.25.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.26.01_00.csv  \n",
            "  inflating: blogData_test-2012.03.27.01_00.csv  \n",
            "  inflating: blogData_test-2012.03.28.01_00.csv  \n",
            "  inflating: blogData_test-2012.03.29.01_00.csv  \n",
            "  inflating: blogData_test-2012.03.30.01_00.csv  \n",
            "  inflating: blogData_test-2012.03.31.01_00.csv  \n",
            "  inflating: blogData_train.csv      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nwpTIrYSl6w2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_url = 'blogData_train.csv'\n",
        "\n",
        "test1url='blogData_test-2012.02.01.00_00.csv'\n",
        "test2url='blogData_test-2012.03.31.01_00.csv'\n",
        "\n",
        "train_ = pd.read_csv(train_url, header=None)\n",
        "df1_ = pd.read_csv(test1url, header=None)\n",
        "df2_ = pd.read_csv(test2url, header=None)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TIJxQ0h-onI8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "2a460b85-dddc-4590-c0f6-f61f95796103"
      },
      "cell_type": "code",
      "source": [
        "def center_and_unitvari(dat): \n",
        "  sc = StandardScaler()\n",
        "  dat_sc = sc.fit_transform(dat)\n",
        "  \n",
        "  newdat = pd.DataFrame(dat_sc)\n",
        "  newdat.columns = newdat.columns+1\n",
        "  assert_almost_equal(newdat.mean().values, 0)\n",
        "  #ssert_almost_equal(newdat.std().values, 1, 2)\n",
        "  return newdat\n",
        "\n",
        "train = center_and_unitvari(train_) \n",
        "df1 = center_and_unitvari(df1_)\n",
        "df2 = center_and_unitvari(df2_)\n",
        "\n",
        "assert all([x==0 for x in train.isna().sum().values + df1.isna().sum().values + df2.isna().sum().values])\n",
        "\n",
        "assert all([all(train.apply(lambda x: np.issubdtype(x, np.number)).values), \n",
        "            all(df1.apply(lambda x: np.issubdtype(x, np.number)).values),\n",
        "            all(df2.apply(lambda x: np.issubdtype(x, np.number)).values),])\n",
        "\n",
        "train.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>...</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.112877</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.138521</td>\n",
              "      <td>-0.139108</td>\n",
              "      <td>0.009598</td>\n",
              "      <td>0.116182</td>\n",
              "      <td>-0.020836</td>\n",
              "      <td>0.368246</td>\n",
              "      <td>-0.119031</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.454696</td>\n",
              "      <td>2.272362</td>\n",
              "      <td>-0.427399</td>\n",
              "      <td>-0.326158</td>\n",
              "      <td>-0.312402</td>\n",
              "      <td>-0.08286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045171</td>\n",
              "      <td>-0.037836</td>\n",
              "      <td>-0.152885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.112877</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.138521</td>\n",
              "      <td>-0.139108</td>\n",
              "      <td>0.009598</td>\n",
              "      <td>0.116182</td>\n",
              "      <td>-0.020836</td>\n",
              "      <td>0.368246</td>\n",
              "      <td>-0.119031</td>\n",
              "      <td>...</td>\n",
              "      <td>2.199274</td>\n",
              "      <td>-0.440071</td>\n",
              "      <td>-0.427399</td>\n",
              "      <td>-0.326158</td>\n",
              "      <td>-0.312402</td>\n",
              "      <td>-0.08286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045171</td>\n",
              "      <td>-0.037836</td>\n",
              "      <td>-0.179406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.112877</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.138521</td>\n",
              "      <td>-0.139108</td>\n",
              "      <td>0.009598</td>\n",
              "      <td>0.116182</td>\n",
              "      <td>-0.020836</td>\n",
              "      <td>0.368246</td>\n",
              "      <td>-0.119031</td>\n",
              "      <td>...</td>\n",
              "      <td>2.199274</td>\n",
              "      <td>-0.440071</td>\n",
              "      <td>-0.427399</td>\n",
              "      <td>-0.326158</td>\n",
              "      <td>-0.312402</td>\n",
              "      <td>-0.08286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045171</td>\n",
              "      <td>-0.037836</td>\n",
              "      <td>-0.179406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.112877</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.138521</td>\n",
              "      <td>-0.139108</td>\n",
              "      <td>0.009598</td>\n",
              "      <td>0.116182</td>\n",
              "      <td>-0.020836</td>\n",
              "      <td>0.368246</td>\n",
              "      <td>-0.119031</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.454696</td>\n",
              "      <td>2.272362</td>\n",
              "      <td>-0.427399</td>\n",
              "      <td>-0.326158</td>\n",
              "      <td>-0.312402</td>\n",
              "      <td>-0.08286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045171</td>\n",
              "      <td>-0.037836</td>\n",
              "      <td>-0.152885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.112877</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.138521</td>\n",
              "      <td>-0.139108</td>\n",
              "      <td>0.009598</td>\n",
              "      <td>0.116182</td>\n",
              "      <td>-0.020836</td>\n",
              "      <td>0.368246</td>\n",
              "      <td>-0.119031</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.454696</td>\n",
              "      <td>2.272362</td>\n",
              "      <td>-0.427399</td>\n",
              "      <td>-0.326158</td>\n",
              "      <td>-0.312402</td>\n",
              "      <td>-0.08286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045171</td>\n",
              "      <td>-0.037836</td>\n",
              "      <td>0.536657</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 281 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        1         2         3         4         5         6         7    \\\n",
              "0  0.010876  0.112877 -0.052468  0.138521 -0.139108  0.009598  0.116182   \n",
              "1  0.010876  0.112877 -0.052468  0.138521 -0.139108  0.009598  0.116182   \n",
              "2  0.010876  0.112877 -0.052468  0.138521 -0.139108  0.009598  0.116182   \n",
              "3  0.010876  0.112877 -0.052468  0.138521 -0.139108  0.009598  0.116182   \n",
              "4  0.010876  0.112877 -0.052468  0.138521 -0.139108  0.009598  0.116182   \n",
              "\n",
              "        8         9         10     ...          272       273       274  \\\n",
              "0 -0.020836  0.368246 -0.119031    ...    -0.454696  2.272362 -0.427399   \n",
              "1 -0.020836  0.368246 -0.119031    ...     2.199274 -0.440071 -0.427399   \n",
              "2 -0.020836  0.368246 -0.119031    ...     2.199274 -0.440071 -0.427399   \n",
              "3 -0.020836  0.368246 -0.119031    ...    -0.454696  2.272362 -0.427399   \n",
              "4 -0.020836  0.368246 -0.119031    ...    -0.454696  2.272362 -0.427399   \n",
              "\n",
              "        275       276      277  278       279       280       281  \n",
              "0 -0.326158 -0.312402 -0.08286  0.0 -0.045171 -0.037836 -0.152885  \n",
              "1 -0.326158 -0.312402 -0.08286  0.0 -0.045171 -0.037836 -0.179406  \n",
              "2 -0.326158 -0.312402 -0.08286  0.0 -0.045171 -0.037836 -0.179406  \n",
              "3 -0.326158 -0.312402 -0.08286  0.0 -0.045171 -0.037836 -0.152885  \n",
              "4 -0.326158 -0.312402 -0.08286  0.0 -0.045171 -0.037836  0.536657  \n",
              "\n",
              "[5 rows x 281 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "M_6zllugl65Z",
        "colab_type": "code",
        "outputId": "843498e4-14bf-4f68-8ceb-88ebc6b857a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "dependent = 281\n",
        "X_train = train.drop(dependent, axis=1)\n",
        "y_train = train[dependent][:, np.newaxis]\n",
        "X_test = df1.drop(dependent, axis=1)\n",
        "y_test = df1[dependent][:, np.newaxis]\n",
        "\n",
        "\n",
        "m = Ridge().fit(X_train,y_train)\n",
        "\n",
        "#m.coef_, m.intercept_, m.n_iter_\n",
        "\n",
        "m.score(X_test, y_test)\n",
        "\n",
        "#m.predict(X_test)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7510771490709747"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "aH9KKpPJl60f",
        "colab_type": "code",
        "outputId": "cc0152ab-7842-4f3c-a782-b6f832fe1951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "cell_type": "code",
      "source": [
        "thresh = np.exp(4)\n",
        "scores = {k: Ridge(alpha=k).fit(X_train,y_train).score(X_test, y_test) for k in np.divide(range(0,1000, 2**4), 100)+np.divide(1,thresh)}\n",
        "\n",
        "scores_df = pd.DataFrame.from_dict(scores, orient='index').reset_index()\n",
        "scores_df.columns = ['alpha', 'R2']\n",
        "\n",
        "C = alt.Chart(scores_df).mark_circle().encode(x='alpha', y='R2')\n",
        "\n",
        "C"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Chart({\n",
              "  data:        alpha        R2\n",
              "  0   0.018316  0.747792\n",
              "  1   0.178316  0.748743\n",
              "  2   0.338316  0.749450\n",
              "  3   0.498316  0.749993\n",
              "  4   0.658316  0.750421\n",
              "  5   0.818316  0.750764\n",
              "  6   0.978316  0.751043\n",
              "  7   1.138316  0.751274\n",
              "  8   1.298316  0.751467\n",
              "  9   1.458316  0.751629\n",
              "  10  1.618316  0.751766\n",
              "  11  1.778316  0.751883\n",
              "  12  1.938316  0.751983\n",
              "  13  2.098316  0.752070\n",
              "  14  2.258316  0.752144\n",
              "  15  2.418316  0.752208\n",
              "  16  2.578316  0.752264\n",
              "  17  2.738316  0.752312\n",
              "  18  2.898316  0.752354\n",
              "  19  3.058316  0.752390\n",
              "  20  3.218316  0.752421\n",
              "  21  3.378316  0.752448\n",
              "  22  3.538316  0.752471\n",
              "  23  3.698316  0.752491\n",
              "  24  3.858316  0.752508\n",
              "  25  4.018316  0.752522\n",
              "  26  4.178316  0.752533\n",
              "  27  4.338316  0.752543\n",
              "  28  4.498316  0.752550\n",
              "  29  4.658316  0.752556\n",
              "  ..       ...       ...\n",
              "  33  5.298316  0.752565\n",
              "  34  5.458316  0.752565\n",
              "  35  5.618316  0.752563\n",
              "  36  5.778316  0.752560\n",
              "  37  5.938316  0.752557\n",
              "  38  6.098316  0.752553\n",
              "  39  6.258316  0.752549\n",
              "  40  6.418316  0.752543\n",
              "  41  6.578316  0.752538\n",
              "  42  6.738316  0.752531\n",
              "  43  6.898316  0.752524\n",
              "  44  7.058316  0.752517\n",
              "  45  7.218316  0.752510\n",
              "  46  7.378316  0.752502\n",
              "  47  7.538316  0.752494\n",
              "  48  7.698316  0.752485\n",
              "  49  7.858316  0.752476\n",
              "  50  8.018316  0.752467\n",
              "  51  8.178316  0.752458\n",
              "  52  8.338316  0.752448\n",
              "  53  8.498316  0.752439\n",
              "  54  8.658316  0.752429\n",
              "  55  8.818316  0.752419\n",
              "  56  8.978316  0.752409\n",
              "  57  9.138316  0.752398\n",
              "  58  9.298316  0.752388\n",
              "  59  9.458316  0.752377\n",
              "  60  9.618316  0.752367\n",
              "  61  9.778316  0.752356\n",
              "  62  9.938316  0.752345\n",
              "  \n",
              "  [63 rows x 2 columns],\n",
              "  encoding: EncodingWithFacet({\n",
              "    x: X({\n",
              "      shorthand: 'alpha'\n",
              "    }),\n",
              "    y: Y({\n",
              "      shorthand: 'R2'\n",
              "    })\n",
              "  }),\n",
              "  mark: 'circle'\n",
              "})"
            ],
            "text/html": [
              "<!DOCTYPE html>\n",
              "<html>\n",
              "<head>\n",
              "  <style>\n",
              "    .vega-actions a {\n",
              "        margin-right: 12px;\n",
              "        color: #757575;\n",
              "        font-weight: normal;\n",
              "        font-size: 13px;\n",
              "    }\n",
              "    .error {\n",
              "        color: red;\n",
              "    }\n",
              "  </style>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega@4\"></script>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega-lite@2.6.0\"></script>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega-embed@3\"></script>\n",
              "</head>\n",
              "<body>\n",
              "  <div id=\"altair-viz\"></div>\n",
              "  <script>\n",
              "      var spec = {\"config\": {\"view\": {\"width\": 400, \"height\": 300}}, \"data\": {\"name\": \"data-9c2c7e6218b8397b91bd97908ec1370a\"}, \"mark\": \"circle\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"alpha\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"R2\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v2.6.0.json\", \"datasets\": {\"data-9c2c7e6218b8397b91bd97908ec1370a\": [{\"alpha\": 0.018315638888734182, \"R2\": 0.7477920463018759}, {\"alpha\": 0.1783156388887342, \"R2\": 0.7487432143772829}, {\"alpha\": 0.33831563888873417, \"R2\": 0.7494503596525561}, {\"alpha\": 0.49831563888873415, \"R2\": 0.7499933570084505}, {\"alpha\": 0.6583156388887342, \"R2\": 0.7504207251367814}, {\"alpha\": 0.8183156388887343, \"R2\": 0.7507636976290027}, {\"alpha\": 0.9783156388887342, \"R2\": 0.7510432781754808}, {\"alpha\": 1.1383156388887343, \"R2\": 0.7512741029010843}, {\"alpha\": 1.2983156388887342, \"R2\": 0.7514666772225014}, {\"alpha\": 1.4583156388887342, \"R2\": 0.7516287326589831}, {\"alpha\": 1.6183156388887343, \"R2\": 0.7517660824496758}, {\"alpha\": 1.7783156388887342, \"R2\": 0.7518831793081004}, {\"alpha\": 1.9383156388887341, \"R2\": 0.7519834895660461}, {\"alpha\": 2.0983156388887343, \"R2\": 0.752069750500399}, {\"alpha\": 2.2583156388887344, \"R2\": 0.7521441512562216}, {\"alpha\": 2.418315638888734, \"R2\": 0.7522084625550078}, {\"alpha\": 2.5783156388887343, \"R2\": 0.7522641313245583}, {\"alpha\": 2.7383156388887344, \"R2\": 0.7523123508279235}, {\"alpha\": 2.898315638888734, \"R2\": 0.7523541133756961}, {\"alpha\": 3.0583156388887343, \"R2\": 0.7523902504622723}, {\"alpha\": 3.2183156388887344, \"R2\": 0.7524214636860966}, {\"alpha\": 3.378315638888734, \"R2\": 0.752448348828701}, {\"alpha\": 3.5383156388887342, \"R2\": 0.7524714147923862}, {\"alpha\": 3.6983156388887344, \"R2\": 0.7524910986305905}, {\"alpha\": 3.858315638888734, \"R2\": 0.7525077775786696}, {\"alpha\": 4.018315638888734, \"R2\": 0.7525217787597769}, {\"alpha\": 4.178315638888734, \"R2\": 0.7525333870713875}, {\"alpha\": 4.3383156388887345, \"R2\": 0.7525428516382083}, {\"alpha\": 4.498315638888735, \"R2\": 0.7525503911259591}, {\"alpha\": 4.658315638888734, \"R2\": 0.7525561981418389}, {\"alpha\": 4.818315638888734, \"R2\": 0.7525604429007839}, {\"alpha\": 4.978315638888734, \"R2\": 0.7525632762952001}, {\"alpha\": 5.138315638888734, \"R2\": 0.7525648324777197}, {\"alpha\": 5.2983156388887345, \"R2\": 0.7525652310454799}, {\"alpha\": 5.458315638888735, \"R2\": 0.7525645788946531}, {\"alpha\": 5.618315638888734, \"R2\": 0.7525629718019268}, {\"alpha\": 5.778315638888734, \"R2\": 0.7525604957782142}, {\"alpha\": 5.938315638888734, \"R2\": 0.7525572282326581}, {\"alpha\": 6.098315638888734, \"R2\": 0.7525532389758669}, {\"alpha\": 6.258315638888734, \"R2\": 0.7525485910883273}, {\"alpha\": 6.418315638888735, \"R2\": 0.7525433416743676}, {\"alpha\": 6.578315638888734, \"R2\": 0.7525375425185743}, {\"alpha\": 6.738315638888734, \"R2\": 0.7525312406595027}, {\"alpha\": 6.898315638888734, \"R2\": 0.75252447889152}, {\"alpha\": 7.058315638888734, \"R2\": 0.7525172962062768}, {\"alpha\": 7.218315638888734, \"R2\": 0.7525097281809288}, {\"alpha\": 7.3783156388887345, \"R2\": 0.7525018073209007}, {\"alpha\": 7.538315638888734, \"R2\": 0.7524935633631596}, {\"alpha\": 7.698315638888734, \"R2\": 0.7524850235452629}, {\"alpha\": 7.858315638888734, \"R2\": 0.7524762128442828}, {\"alpha\": 8.018315638888733, \"R2\": 0.752467154189515}, {\"alpha\": 8.178315638888733, \"R2\": 0.7524578686526077}, {\"alpha\": 8.338315638888734, \"R2\": 0.7524483756171153}, {\"alpha\": 8.498315638888734, \"R2\": 0.7524386929306474}, {\"alpha\": 8.658315638888734, \"R2\": 0.7524288370413107}, {\"alpha\": 8.818315638888734, \"R2\": 0.752418823120307}, {\"alpha\": 8.978315638888734, \"R2\": 0.7524086651724011}, {\"alpha\": 9.138315638888733, \"R2\": 0.7523983761353165}, {\"alpha\": 9.298315638888733, \"R2\": 0.7523879679697925}, {\"alpha\": 9.458315638888733, \"R2\": 0.7523774517408368}, {\"alpha\": 9.618315638888733, \"R2\": 0.7523668376914228}, {\"alpha\": 9.778315638888733, \"R2\": 0.7523561353091575}, {\"alpha\": 9.938315638888733, \"R2\": 0.7523453533871183}]}};\n",
              "      var embedOpt = {\"mode\": \"vega-lite\"};\n",
              "\n",
              "      function showError(el, error){\n",
              "          el.innerHTML = ('<div class=\"error\" style=\"color:red;\">'\n",
              "                          + '<p>JavaScript Error: ' + error.message + '</p>'\n",
              "                          + \"<p>This usually means there's a typo in your chart specification. \"\n",
              "                          + \"See the javascript console for the full traceback.</p>\"\n",
              "                          + '</div>');\n",
              "          throw error;\n",
              "      }\n",
              "      const el = document.getElementById('altair-viz');\n",
              "      vegaEmbed(\"#altair-viz\", spec, embedOpt)\n",
              "        .catch(error => showError(el, error));\n",
              "\n",
              "  </script>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "8itHJxPvl67V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# doesn't seem to matter. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x-S80gEtl629",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OCKwBFoUl6zM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d418984c-f16c-434d-9a73-d074a396e10a"
      },
      "cell_type": "code",
      "source": [
        "ols = LinearRegression()\n",
        "ols.fit(X_train,y_train).score(X_test,y_test) # that's nuts. "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2.2448258545033072e+20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "TT47E5gNl6rq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_y7ZtKfWl6qJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}